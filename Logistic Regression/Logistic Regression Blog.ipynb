{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>female</th>\n",
       "      <th>ses</th>\n",
       "      <th>schtyp</th>\n",
       "      <th>prog</th>\n",
       "      <th>read</th>\n",
       "      <th>write</th>\n",
       "      <th>math</th>\n",
       "      <th>science</th>\n",
       "      <th>socst</th>\n",
       "      <th>honors</th>\n",
       "      <th>awards</th>\n",
       "      <th>cid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.0</td>\n",
       "      <td>female</td>\n",
       "      <td>low</td>\n",
       "      <td>public</td>\n",
       "      <td>vocation</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>not enrolled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108.0</td>\n",
       "      <td>male</td>\n",
       "      <td>middle</td>\n",
       "      <td>public</td>\n",
       "      <td>general</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>not enrolled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>male</td>\n",
       "      <td>high</td>\n",
       "      <td>public</td>\n",
       "      <td>vocation</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>not enrolled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.0</td>\n",
       "      <td>male</td>\n",
       "      <td>low</td>\n",
       "      <td>public</td>\n",
       "      <td>vocation</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>not enrolled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153.0</td>\n",
       "      <td>male</td>\n",
       "      <td>middle</td>\n",
       "      <td>public</td>\n",
       "      <td>vocation</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>not enrolled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  female     ses  schtyp      prog  read  write  math  science  socst  \\\n",
       "0   45.0  female     low  public  vocation  34.0   35.0  41.0     29.0   26.0   \n",
       "1  108.0    male  middle  public   general  34.0   33.0  41.0     36.0   36.0   \n",
       "2   15.0    male    high  public  vocation  39.0   39.0  44.0     26.0   42.0   \n",
       "3   67.0    male     low  public  vocation  37.0   37.0  42.0     33.0   32.0   \n",
       "4  153.0    male  middle  public  vocation  39.0   31.0  40.0     39.0   51.0   \n",
       "\n",
       "         honors  awards  cid  \n",
       "0  not enrolled     0.0    1  \n",
       "1  not enrolled     0.0    1  \n",
       "2  not enrolled     0.0    1  \n",
       "3  not enrolled     0.0    1  \n",
       "4  not enrolled     0.0    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df = pd.read_stata('./hsbdemo.dta')\n",
    "orig_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fake dataset contains variables on 200 students. Our task is to predict `prog` or program type for each student. To simplify the problem, I will use two independent variables: `ses` or socio-economic status, and `write`, or the student's writing score.\n",
    "\n",
    "- `prog` is the dependent variable. There are three levels: vocation, general, academic\n",
    "- `ses` is an ordinal independent variable with three levels: low, middle, high\n",
    "- `write` is a continuous independent variable\n",
    "\n",
    "I'm going to implement both binary logistic regression and multinomial logistic regression. Thus, there will be two parts to this blog.\n",
    "\n",
    "Before I implement anything, however, I will preprocess the data by encoding `ses` and `prog`.\n",
    "\n",
    "### Encoding\n",
    "Why do we do this? To help our model in reading the data. Machine learning algorithms are built to take in numbers, not strings. So, we have to **encode** the data into numbers. This can be done using **one-hot-encoding**, which creates a boolean variable for each level of `ses`. Pandas' `get_dummies` function handles this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.get_dummies(orig_df, columns = ['ses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `prog`, I will encode it like so:\n",
    "- $y = 0$ when the student's program type is general. \n",
    "- $y = 1$ when the student's program type is vocational.  \n",
    "- $y = 2$ when the student's program type is academic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.replace({'prog': {'general': 0, 'vocation': 1, 'academic': 2}}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start with implementing binary logistic regression. To do this, I'll remove all rows where `prog` is academic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "$$h_{\\theta}(x) = \\sigma(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}$$\n",
    "\n",
    "### Cost Function\n",
    "$$J(\\theta) = \\frac{1}{n}[\\sum_{i = 1}^{n}-y^ilog(h_\\theta(x^i))+(1 - y^i)log(1 - h_\\theta(x^i))]$$\n",
    "\n",
    "where $n$ is the number of training samples.\n",
    "\n",
    "### Deriving Cost Function\n",
    "$$\\frac{\\partial{J(\\theta)}}{\\partial\\theta} = \\frac{1}{n}(h_\\theta(x) - y)x$$\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha\\frac{\\partial{J(\\theta)}}{\\partial\\theta}$$\n",
    "\n",
    "## Implementation from Scratch\n",
    "\n",
    "I will vectorize the implementations to speed up performance.\n",
    "\n",
    "- $X$: (95, 4)\n",
    "- $Y$: (95, 1)\n",
    "- $\\theta$: (4, 1)\n",
    "\n",
    "$$h = \\sigma(X\\theta)$$\n",
    "$$J(\\theta) = \\frac{1}{n} \\cdot (-y^Tlog(h)+(1 - y^T)log(1 - h))$$\n",
    "$$\\frac{\\partial{J(\\theta)}}{\\partial\\theta} = X^T\\cdot(preds - Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prog</th>\n",
       "      <th>ses_low</th>\n",
       "      <th>ses_middle</th>\n",
       "      <th>ses_high</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prog  ses_low  ses_middle  ses_high  write\n",
       "0     1        1           0         0   35.0\n",
       "1     0        0           1         0   33.0\n",
       "2     1        0           0         1   39.0\n",
       "3     1        1           0         0   37.0\n",
       "4     1        0           1         0   31.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_df = orig_df[['prog', 'ses_low', 'ses_middle', 'ses_high', 'write']]\n",
    "bi_df = bi_df[bi_df['prog'] != 2]\n",
    "print(bi_df.shape)\n",
    "bi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def bi_hypothesis(X, theta):\n",
    "    \"\"\"\n",
    "    Returns an array of probabilities that the class label is 1.\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, theta))\n",
    "\n",
    "def bi_cost_function(preds, Y, N):\n",
    "    \"\"\"\n",
    "    Computes the cost function for binary logistic regression.\n",
    "    \"\"\"\n",
    "    class1_cost = -Y * np.log(preds)\n",
    "    class2_cost = (1 - Y) * np.log(1 - preds)\n",
    "    \n",
    "    cost = class1_cost + class2_cost\n",
    "    \n",
    "    return cost.sum() / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are four features -> four parameters\n",
    "theta = np.array([0.0, 0.0, 0.0, 0.0]).reshape(4, 1)\n",
    "\n",
    "# Start training\n",
    "X = bi_df[['ses_low', 'ses_middle', 'ses_high', 'write']]\n",
    "Y = np.array(bi_df['prog']).reshape(len(X), 1)\n",
    "N = len(X)\n",
    "\n",
    "n_iters = 10000\n",
    "alpha = 0.00009\n",
    "\n",
    "costs = []\n",
    "\n",
    "for i in range(n_iters):\n",
    "    ### Predict\n",
    "    preds = bi_hypothesis(X, theta)\n",
    "\n",
    "    ### Calculate cost\n",
    "    cost = bi_cost_function(preds, Y, N)\n",
    "    costs.append(cost)\n",
    "\n",
    "    ### Gradient Descent\n",
    "    gradient = alpha * (1.0 / N) * np.dot(X.T, preds - Y)\n",
    "\n",
    "    theta -= gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAcnUlEQVR4nO3df5Cd1X3f8ffn3rs/hFYSIDYulhgkBkotaA1kEVBTT2sSRzgZa5iBsZimUVMyDCV0nKQzGanpMAl/lUwnOJ7QBMaQUtUxuIrdbhglNGPsdtrJKCy1sCQLxYssm40grCyh36vdlb794567evbu2d2r1Yr98XxeMzv7POc5z73P2Qv3o3PO80MRgZmZlVNlrg/AzMzmjkPAzKzEHAJmZiXmEDAzKzGHgJlZidXm+gAuxjXXXBNr1qyZ68MwM1sw3nzzzcMR0T3Z9gUVAmvWrKGvr2+uD8PMbMGQ9KOptns4yMysxBwCZmYl5hAwMysxh4CZWYk5BMzMSswhYGZWYg4BM7MSK0UIfPlbP+B//c3gXB+Gmdm8U4oQ+E/f6ef/9h+e68MwM5t3ShECQvjhOWZmE5UjBATOADOzicoRAoAzwMxsonKEgOSegJlZRjlCAAj3BczMJihHCHhOwMwsqyQh4LODzMxyShICnhg2M8spRwjg4SAzs5xyhIDkiWEzs4xyhADuCZiZ5bQUApI2SNovqV/Slsz2DkmvpO07Ja1J5esl7Uo/b0l6oLDPlZK2S3pb0j5J98xWoyYen+cEzMxypg0BSVXgWeB+YB3wsKR1TdUeAY5GxI3AM8DTqXwP0BMRtwEbgOck1dK23wf+IiL+AfBJYN+lNmaKVrgnYGaW0UpPYD3QHxEHImIYeBnY2FRnI/BSWt4O3CdJEXE6IkZTeSfpH+SSlgOfBl4AiIjhiPjw0poyOQncFzAzm6iVEFgFvFtYH0hl2TrpS/8YsBJA0l2S9gK7gcfS9huAQeCPJX1X0lckLc29uaRHJfVJ6hscnNkzATwnYGaW10oIKFPW/JU6aZ2I2BkRtwB3AlsldQI14A7gDyPiduAUMGGuIe3/fET0RERPd3d3C4ebaYCvGDYzy2olBAaA6wrrq4FDk9VJY/4rgCPFChGxj/qX/a2p/kBE7Eybt1MPhctC+BRRM7OcVkLgDeAmSWsltQObgN6mOr3A5rT8IPB6RETapwYg6XrgZuBgRLwPvCvp5rTPfcD3L7Etk3JPwMwsrzZdhYgYlfQE8BpQBV6MiL2SngL6IqKX+gTvNkn91HsAm9Lu9wJbJI0A54HHI6LxnMd/A3w1BcsB4Jdns2FFfp6AmVnetCEAEBE7gB1NZU8WloeAhzL7bQO2TfKau4CeiznYmfLzBMzM8spxxbD8PAEzs5zyhIAzwMxsgnKEAH6egJlZTjlCwPcOMjPLKkcI4OEgM7OccoSA5J6AmVlGOUIAPCdgZpZRihDAcwJmZlmlCAHfSdrMLK8cIeBnDJuZZZUjBPDZQWZmOeUIAV8xbGaWVY4Q8PMEzMyyyhEC7gmYmWWVIgTAJweZmeWUIgQqfp6AmVlWKUKgPhzkFDAza1aeEJjrgzAzm4fKEQJ+noCZWVY5QsA9ATOzrHKEAD5F1MwspxQhgJ8nYGaW1VIISNogab+kfklbMts7JL2Stu+UtCaVr5e0K/28JemBwj4HJe1O2/pmq0HZ48dnB5mZ5dSmqyCpCjwL/CwwALwhqTcivl+o9ghwNCJulLQJeBr4ArAH6ImIUUnXAm9J+rOIGE37/bOIODybDcq34XK/g5nZwtRKT2A90B8RByJiGHgZ2NhUZyPwUlreDtwnSRFxuvCF38kczc96TsDMLK+VEFgFvFtYH0hl2TrpS/8YsBJA0l2S9gK7gccKoRDA/5T0pqRHJ3tzSY9K6pPUNzg42Eqbcq/hG8iZmWW0EgK5wZTmb9RJ60TEzoi4BbgT2CqpM23/VETcAdwP/KqkT+fePCKej4ieiOjp7u5u4XDzDXBPwMxsolZCYAC4rrC+Gjg0WR1JNWAFcKRYISL2AaeAW9P6ofT7A+Cb1IedLgvfRdTMLK+VEHgDuEnSWkntwCagt6lOL7A5LT8IvB4RkfapAUi6HrgZOChpqaRlqXwp8Fnqk8iXhZ8nYGaWN+3ZQenMnieA14Aq8GJE7JX0FNAXEb3AC8A2Sf3UewCb0u73AlskjQDngccj4rCkG4Bvqn7aTg34k4j4i9lu3Bj3BMzMsqYNAYCI2AHsaCp7srA8BDyU2W8bsC1TfgD45MUe7ExVBOcdAmZmE5TiimHfQM7MLK8cIeDhIDOzrPKEwFwfhJnZPFSOEPBwkJlZVjlCwD0BM7OsUoQAeE7AzCynFCEgP0/AzCyrHCEA7gqYmWWUIwQ8J2BmllWOEMAdATOznHKEgJ8nYGaWVY4QwD0BM7OccoSAbxthZpZVkhDwKaJmZjnlCAHwbSPMzDLKEQIeDjIzyypHCPjxkmZmWeUIAfcEzMyyyhMCc30QZmbzUDlCwM8TMDPLKkUI4J6AmVlWKUKgfhfRuT4KM7P5p6UQkLRB0n5J/ZK2ZLZ3SHolbd8paU0qXy9pV/p5S9IDTftVJX1X0quz0Zgpjt8ZYGaWMW0ISKoCzwL3A+uAhyWta6r2CHA0Im4EngGeTuV7gJ6IuA3YADwnqVbY74vAvktrwvR8sZiZWV4rPYH1QH9EHIiIYeBlYGNTnY3AS2l5O3CfJEXE6YgYTeWdFAZlJK0Gfh74yqU0oBU+O8jMLK+VEFgFvFtYH0hl2TrpS/8YsBJA0l2S9gK7gccKofAl4DeB81O9uaRHJfVJ6hscHGzhcDOvga8TMDPLaSUElClr/kqdtE5E7IyIW4A7ga2SOiX9AvBBRLw53ZtHxPMR0RMRPd3d3S0c7kR+noCZWV4rITAAXFdYXw0cmqxOGvNfARwpVoiIfcAp4FbgU8DnJR2kPrz0GUn/dQbH3xL3BMzM8loJgTeAmyStldQObAJ6m+r0ApvT8oPA6xERaZ8agKTrgZuBgxGxNSJWR8Sa9HqvR8QvzkJ7siQ5BMzMMmrTVYiIUUlPAK8BVeDFiNgr6SmgLyJ6gReAbZL6qfcANqXd7wW2SBqhPvb/eEQcvhwNmUr93kFOATOzZtOGAEBE7AB2NJU9WVgeAh7K7LcN2DbNa38H+E4rxzFTwmcHmZnllOOKYd9F1Mwsqxwh4OcJmJlllSME3BMwM8sqTwjM9UGYmc1DpQgB8CmiZmY5pQgB+V7SZmZZ5QgBPCdgZpZTjhDwnICZWVY5QsDPGDYzyypHCLgnYGaWVY4QwHMCZmY55QgBeTjIzCynJCHgnoCZWU4pQqAqcd4pYGY2QSlCoFIR550BZmYTlCIEJDjnnoCZ2QSlCIGqJ4bNzLJKEQIVeTjIzCynJCGAJ4bNzDJKEQL16wT8sHkzs2alCIFqRQAeEjIza1KKEEgZ4CEhM7MmLYWApA2S9kvql7Qls71D0itp+05Ja1L5ekm70s9bkh5I5Z2S/jqV7ZX0O7PZqMzxAQ4BM7Nm04aApCrwLHA/sA54WNK6pmqPAEcj4kbgGeDpVL4H6ImI24ANwHOSasBZ4DMR8UngNmCDpLtno0E5lUYInL9c72BmtjC10hNYD/RHxIGIGAZeBjY21dkIvJSWtwP3SVJEnI6I0VTeSbqjc9SdTOVt6eey/TO9mlrpnoCZ2XithMAq4N3C+kAqy9ZJX/rHgJUAku6StBfYDTzWCAVJVUm7gA+Av4yInbk3l/SopD5JfYODg623rKDi4SAzs6xWQkCZsuZv00nrRMTOiLgFuBPYKqkzlZ9Lw0SrgfWSbs29eUQ8HxE9EdHT3d3dwuFmGuDhIDOzrFZCYAC4rrC+Gjg0WZ005r8COFKsEBH7gFPArU3lHwLfoT5ncFlUfXaQmVlWKyHwBnCTpLWS2oFNQG9TnV5gc1p+EHg9IiLtUwOQdD1wM3BQUrekK1P5EuBngLcvvTl5lYqHg8zMcmrTVYiIUUlPAK8BVeDFiNgr6SmgLyJ6gReAbZL6qfcANqXd7wW2SBoBzgOPR8RhSf8IeCmdeVQBvh4Rr85665ILp4herncwM1uYpg0BgIjYAexoKnuysDwEPJTZbxuwLVP+PeD2iz3YmfLFYmZmeaW4Yrjqs4PMzLJKEQIVDweZmWWVIgTUGA5yCpiZjVOKEPDFYmZmeaUIAd9K2swsrxQhIJ8dZGaWVYoQuHAXUYeAmVlRKULAw0FmZnmlCAFfLGZmlleKEPCTxczM8koRAn6ymJlZXilCwE8WMzPLK0UIeDjIzCyvFCHgeweZmeWVJATqv90TMDMbrxQhUPXFYmZmWaUIAT9ZzMwsrxQh4OEgM7O8coSAHzRvZpZVjhDwcJCZWVZJQqD+2z0BM7PxShICPjvIzCynpRCQtEHSfkn9krZktndIeiVt3ylpTSpfL2lX+nlL0gOp/DpJ35a0T9JeSV+czUY1862kzczypg0BSVXgWeB+YB3wsKR1TdUeAY5GxI3AM8DTqXwP0BMRtwEbgOck1YBR4N9GxCeAu4FfzbzmrPGTxczM8lrpCawH+iPiQEQMAy8DG5vqbAReSsvbgfskKSJOR8RoKu8EAiAi3ouI/5eWTwD7gFWX1pTJNYaDwiFgZjZOKyGwCni3sD7AxC/ssTrpS/8YsBJA0l2S9gK7gccKoUDavga4HdiZe3NJj0rqk9Q3ODjYwuFO1AiBc76VtJnZOK2EgDJlzf+knrROROyMiFuAO4GtkjrHdpK6gD8Ffi0ijufePCKej4ieiOjp7u5u4XAnaswJjPqBAmZm47QSAgPAdYX11cChyeqkMf8VwJFihYjYB5wCbk312qgHwFcj4hszOfhWtVUbPQEPB5mZFbUSAm8AN0laK6kd2AT0NtXpBTan5QeB1yMi0j41AEnXAzcDB1W/mc8LwL6I+L3ZaMhUxnoC5xwCZmZFtekqRMSopCeA14Aq8GJE7JX0FNAXEb3Uv9C3Seqn3gPYlHa/F9giaQQ4DzweEYcl3Qv8C2C3pF2p7r+LiB2z2rqkLT1abMTDQWZm40wbAgDpy3lHU9mTheUh4KHMftuAbZny/0N+HuGyqLknYGaWVYorhmupJzDqOQEzs3HKEQJjPQEPB5mZFZUjBKqNU0TdEzAzKypFCLRV0sSwewJmZuOUIgQqFVGRJ4bNzJqVIgQAapWKh4PMzJqUJwSq8sSwmVmT8oRARe4JmJk1KU0ItFUrnhg2M2tSmhCoVuQbyJmZNSlNCNR7Ag4BM7Oi0oRArSo/T8DMrEl5QqAiXydgZtakRCFQcU/AzKxJeUKg6p6AmVmzEoVAhRGfHWRmNk5pQqCjWmF49NxcH4aZ2bxSmhDobK8yNOI5ATOzotKEwJK2CkMj7gmYmRWVKASqnHEImJmNU54QaK9yZtghYGZWVJoQ6HRPwMxsgpZCQNIGSfsl9UvaktneIemVtH2npDWpfL2kXennLUkPFPZ5UdIHkvbMVmOmsqSt6jkBM7Mm04aApCrwLHA/sA54WNK6pmqPAEcj4kbgGeDpVL4H6ImI24ANwHOSamnbf05lH4klbVVGzoVvJ21mVtBKT2A90B8RByJiGHgZ2NhUZyPwUlreDtwnSRFxOiJGU3knMHa1VkT8b+DIJR39RVjSXgVwb8DMrKCVEFgFvFtYH0hl2TrpS/8YsBJA0l2S9gK7gccKodASSY9K6pPUNzg4eDG7jtPZVg8BzwuYmV3QSggoU9Z8/4VJ60TEzoi4BbgT2Cqp82IOMCKej4ieiOjp7u6+mF3H6eqoj0KdGLqoDDIzW9RaCYEB4LrC+mrg0GR10pj/CpqGeiJiH3AKuHWmB3sprl7aDsDRU8Nz8fZmZvNSKyHwBnCTpLWS2oFNQG9TnV5gc1p+EHg9IiLtUwOQdD1wM3BwVo78IjVC4PBJh4CZWcO0IZDG8J8AXgP2AV+PiL2SnpL0+VTtBWClpH7gN4DGaaT3Am9J2gV8E3g8Ig4DSPoa8FfAzZIGJD0ymw1rtrKrHgJH3BMwMxtTm74KRMQOYEdT2ZOF5SHgocx+24Btk7zmwxd1pJeo0RM4cursR/m2ZmbzWmmuGO6oVbmmq50fHzk914diZjZvlCYEAP7+x5ax//0Tc30YZmbzRqlC4B+uWsG+907w4WnPC5iZQclC4PO3fZzhc+f59Vd28er3DvFX7/yEt98/znvHznDy7CgRfvykmZVLSxPDi8UtH1/Bv//5T/C7r+3n2/snXn1cUf2isuVL2ljW2cayzhrLO9tY3lljWWeNZZ1tLF9SG7dtrLyzvl9HrYKUu3bOzGz+KVUIAPzKP7mBX7z7en54+BRHTw1z9PQIx4dGODE0womhUY6fSb+HRjkxNMLffniGt9O2E0MjTPes+raqxkJh2VhINAIjhceSRog0AuRCvWWdbbTXStVBM7M5VLoQgPp9hD5x7fKL3i8iODV8LhMYI2Oh0QiL42curP/w8KlUPsrJs9PftqKzrTKht7G8GCAdtUKYjA+a5Z1tdHXWqFbcGzGz6ZUyBGZKEl0dNbo6aly7Ymavce58cDIFx4nC73HBcvZCkDS2H/rwzFjQDI1Mfzvsro7auN5FsWfS6ImMDWMVh7XScNfS9qqHtcxKwCHwEatWxIor2lhxRduMX2N49Hyh15ECY6w3cqGHcqIQNIdPDo/1SI4PjTBybupxrZnPj1woW9LmIDGb7xwCC1B7rcLKrg5WdnXMaP+I4Ozo+XpwnBkdFyizOT9Sq4iuRjB0tDX1SmppW9u44FjeVOYeidnl5RAoIUl0tlXpbKvyU8tm9hpTzY9cmP9o7q2MXnSQNHokyzI9jWVNgVHspVwYDqvPoVQ8R2KW5RCwGZmN+ZGI4PTwubHAOF4IjPG/x8+dfHBiiHcGL9SdbmgLJs6RjPvdMUl5YZirq6NGreqztmzxcQjYnJHE0o4aSztq1J8+evGKQ1snpgiRcetnRzhyapgf/eT0WA9leHT6yfYr2qvjexiTzIWMDxef/mvzm0PAFrTZGNoCODuaeiRNw1fNE/CNEGmUHfrwzNhyK48u7ahVJgxdNXoauR5IbujLFyTabHIImFG/y2xHV5VrZjjZDjBy7vxYiBSHr06eHR8izdeU/N3xobHlU8PTB0njgsSxcOiYfHK9a5Jyn7llDQ4Bs1nSVq1w1dJ2rkrPrpiJc+cjhUZ+eKs4b1IMlx8fOT0WPvX7YE39PtWKMiEy+aT7hHDpqLG03RPui4FDwGweqVbEiiVtrFgy8+tIzp8PTg3n5kImmy9pnLk1xImhE2Phcm6aU7fUuJakKTga8yZdaVhrbD0zR9LVUfM8yRxzCJgtMpVKY7ho5kESEZwZOZftgUw16T544iwHBk+OhU4rE+71eZLmU3trdI3roVxY70q9kuJ290pmziFgZhNI4or2Gle01/jY8pmduQX1CffGPMnJs2m4qrA+vpcyysm0Xj9z68Kw13TXk0jQ1V4buzix0eu4EBiTT8IX18s46e4QMLPLpjHhPtOr2+HChYknGz2OscC4EBLHC+uNgPnw9DDvHj09VreVs7faqmopMLrG5kgKPZQ0NLa0o7qgrilxCJjZvFa8MPHvrZh5r2Tk3HlOnZ04hDXWI5kkXA59eGbcacHTzZXA+GtKuoq3SumYOOyVW/8oz+BqKQQkbQB+H6gCX4mI/9C0vQP4L8BPAz8BvhARByWtB55vVAN+OyK+2cprmpnNprZqhSuvaOfKK2Z+9lZEMDRyfiwULgxtjRR6I01nb6WQef/Y0FjotHJL+WrlQvitunIJX3/snhkf91SmDQFJVeBZ4GeBAeANSb0R8f1CtUeAoxFxo6RNwNPAF4A9QE9EjEq6FnhL0p8B0cJrmpnNK5JY0l5lSfulXZzYOBW40QsZC4+m9cY8SvtlHF5qpSewHuiPiAMAkl4GNgLFL+yNwG+n5e3AH0hSRJwu1Omk/uXf6muamS1K408FXjKnx9JKvKwC3i2sD6SybJ2IGAWOASsBJN0laS+wG3gsbW/lNc3M7DJrJQRyMxPNMyOT1omInRFxC3AnsFVSZ4uvWX9h6VFJfZL6BgcnPhzezMxmrpUQGACuK6yvBg5NVkdSDVgBHClWiIh9wCng1hZfs7Hf8xHRExE93d3dLRyumZm1qpUQeAO4SdJaSe3AJqC3qU4vsDktPwi8HhGR9qkBSLoeuBk42OJrmpnZZTbtxHA6s+cJ4DXqp3O+GBF7JT0F9EVEL/ACsE1SP/UewKa0+73AFkkjwHng8Yg4DJB7zVlum5mZTUMx3e0G55Genp7o6+ub68MwM1swJL0ZET2TbV841zabmdmscwiYmZXYghoOkjQI/GiGu18DHJ7Fw1kI3ObFr2ztBbf5Yl0fEZOeWrmgQuBSSOqbalxsMXKbF7+ytRfc5tnm4SAzsxJzCJiZlViZQuD56assOm7z4le29oLbPKtKMydgZmYTlaknYGZmTRwCZmYltuhDQNIGSfsl9UvaMtfHcykkXSfp25L2Sdor6Yup/GpJfynpB+n3Valckr6c2v49SXcUXmtzqv8DSZsne8/5QFJV0nclvZrW10ramY79lXQTQiR1pPX+tH1N4TW2pvL9kn5ublrSOklXStou6e30ed+zmD9nSb+e/pveI+lrkjoX4+cs6UVJH0jaUyibtc9V0k9L2p32+bLUwkOKI2LR/lC/Od07wA1AO/AWsG6uj+sS2nMtcEdaXgb8DbAO+F1gSyrfAjydlj8H/Dn15zfcDexM5VcDB9Lvq9LyVXPdvina/RvAnwCvpvWvA5vS8h8B/zotPw78UVreBLySltelz74DWJv+m6jOdbumafNLwK+k5XbgysX6OVN/oNQPgSWFz/dfLsbPGfg0cAewp1A2a58r8NfAPWmfPwfun/aY5vqPcpn/4PcArxXWtwJb5/q4ZrF9/4P6c5r3A9emsmuB/Wn5OeDhQv39afvDwHOF8nH15tMP9WdNfAv4DPBq+o/7MFBr/oyp35X2nrRcS/XU/LkX683HH2B5+lJUU/mi/Jy58KTBq9Pn9irwc4v1cwbWNIXArHyuadvbhfJx9Sb7WezDQYv2MZapC3w7sBP4WES8B5B+/1SqNln7F9Lf5UvAb1K/FTnUH1v6YdQfUwrjj32yx5wupPZCvec6CPxxGgb7iqSlLNLPOSL+FviPwI+B96h/bm+y+D/nhtn6XFel5ebyKS32EGj5MZYLiaQu4E+BX4uI41NVzZTFFOXziqRfAD6IiDeLxZmqMc22BdHeghr1IYM/jIjbqT+Rb6r5rAXd7jQGvpH6EM7HgaXA/Zmqi+1zns7FtnNG7V/sIdDyYywXCklt1APgqxHxjVT8d5KuTduvBT5I5ZO1f6H8XT4FfF7SQeBl6kNCXwKuVHpiHeOPfbLHnC6U9jYMAAMRsTOtb6ceCov1c/4Z4IcRMRgRI8A3gH/M4v+cG2brcx1Iy83lU1rsIbCoHmOZZvpfAPZFxO8VNhUf77mZ+lxBo/yX0lkGdwPHUnfzNeCzkq5K/wr7bCqbVyJia0Ssjog11D+71yPinwPfpv4YU5jY3gmPOU3lm9JZJWuBm6hPoM1LEfE+8K6km1PRfcD3WaSfM/VhoLslXZH+G2+0d1F/zgWz8rmmbSck3Z3+jr9UeK3JzfUkyUcwCfM56mfRvAP81lwfzyW25V7q3bvvAbvSz+eoj4d+C/hB+n11qi/g2dT23UBP4bX+FdCffn55rtvWQtv/KRfODrqB+v/c/cB/AzpSeWda70/bbyjs/1vp77CfFs6YmOsf4DagL33W/536WSCL9nMGfgd4G9gDbKN+hs+i+5yBr1Gf9xih/i/3R2bzcwV60t/wHeAPaDq5IPfj20aYmZXYYh8OMjOzKTgEzMxKzCFgZlZiDgEzsxJzCJiZlZhDwMysxBwCZmYl9v8B3++CvHyC+MYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(n_iters)), costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01848842]\n",
      " [ 0.04881776]\n",
      " [-0.009395  ]\n",
      " [-0.00012962]]\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.649232\n",
      "         Iterations 5\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   prog   No. Observations:                   95\n",
      "Model:                          Logit   Df Residuals:                       91\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Fri, 26 Jun 2020   Pseudo R-squ.:                 0.06148\n",
      "Time:                        22:06:23   Log-Likelihood:                -61.677\n",
      "converged:                       True   LL-Null:                       -65.717\n",
      "Covariance Type:            nonrobust   LLR p-value:                   0.04437\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.2091      1.195      1.849      0.065      -0.133       4.551\n",
      "ses_middle     0.7408      0.490      1.512      0.131      -0.220       1.701\n",
      "ses_high       0.2205      0.659      0.335      0.738      -1.071       1.512\n",
      "write         -0.0517      0.024     -2.199      0.028      -0.098      -0.006\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exclude ses_low to make this column a baseline. \n",
    "# This prevents multicollinearity.\n",
    "X = bi_df[['ses_middle', 'ses_high', 'write']]\n",
    "\n",
    "y = bi_df['prog']\n",
    "\n",
    "bi_model = sm.Logit(y, sm.add_constant(X))\n",
    "bi_result = bi_model.fit()\n",
    "\n",
    "print(bi_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the results\n",
    "\n",
    "There's a lot to unpack here, so let's go one by one. \n",
    "\n",
    "#### Logit Recap\n",
    "In linear regression, it was easy to interpret what the coefficients meant. Logistic regression is a bit more difficult, as it requires us to understand what the log-odds are.\n",
    "\n",
    "Recall from the [first part](https://philliplagoc.wordpress.com/2020/06/24/my-take-on-logistic-regression-part-1/) that the log-odds is computed by:\n",
    "$$logit(p) = log\\frac{p}{1 - p}$$\n",
    "\n",
    "Again, we are simply taking the log of the odds (log-odds!), which allows us to transform the range from $[0, 1]$, which was what the hypothesis outputs, to a range from $[-\\infty, \\infty]$. There are other ways to do this transformation e.g., probit, but I'll cover that another time. \n",
    "\n",
    "In essence, **logistic regression models the logit-transformed probability as a linear relationship with the independent variables**:\n",
    "\n",
    "$$logit(p) = log\\frac{p}{1 - p} = \\beta_0 + \\beta_1X_1 + \\beta_2X_2$$\n",
    "\n",
    "Solving for $p$ brings us right back to the sigmoid function!\n",
    "\n",
    "$$\\frac{1-p}{p} = \\frac{1}{e^{\\beta_0 + \\beta_1X_1 + \\beta_2X_2}}$$\n",
    "\n",
    "$$\\frac{1}{p} = 1 + \\frac{1}{e^{\\beta_0 + \\beta_1X_1 + \\beta_2X_2}}$$\n",
    "\n",
    "$$\\frac{1}{p} = \\frac{1 + e^{\\beta_0 + \\beta_1X_1 + \\beta_2X_2}}{e^{\\beta_0 + \\beta_1X_1 + \\beta_2X_2}} = \\sigma(\\theta^Tx)$$\n",
    "\n",
    "$$p = \\frac{e^{\\beta_0 + \\beta_1X_1 + \\beta_2X_2}}{1 + e^{\\beta_0 + \\beta_1X_1 + \\beta_2X_2}} = \\sigma(\\theta^Tx)$$\n",
    "\n",
    "#### What do the coefficients mean?\n",
    "\n",
    "**Note: [This](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/) article was immensely helpful in understanding what these coefficients meant. Check [this](http://blog.yhat.com/posts/logistic-regression-and-python.html) article, too!**\n",
    "\n",
    "Each coefficient estimate is the expected change in the student's log-odds in the vocational program for each unit increase in the corresponding independent variable while holding the other variables constant.\n",
    "\n",
    "For every unit increase in the writing score, the log-odds will change by -0.05. The idea is the same as the other variables.\n",
    "\n",
    "Exponentiating these coefficient estimates will give us the odds of the student being in the vocational program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const         9.107444\n",
      "ses_middle    2.097511\n",
      "ses_high      1.246751\n",
      "write         0.949620\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(bi_result.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can expect the odds of a student being in the vocational program to **decrease** by about 0.95% for every unit increase in their writing score. \n",
    "\n",
    "We can also expect that, if the student has a socioeconomic status categorized as 'middle', the odds of the student being in the vocational program **increases** by about 109%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression\n",
    "\n",
    "I will delve into how to implement multinomial logistic regression by hand sometime in the future. For now, `statsmodels` can let us solve multinomial logistic regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prog</th>\n",
       "      <th>ses_low</th>\n",
       "      <th>ses_middle</th>\n",
       "      <th>ses_high</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prog  ses_low  ses_middle  ses_high  write\n",
       "0     1        1           0         0   35.0\n",
       "1     0        0           1         0   33.0\n",
       "2     1        0           0         1   39.0\n",
       "3     1        1           0         0   37.0\n",
       "4     1        0           1         0   31.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_df = orig_df[['prog', 'ses_low', 'ses_middle', 'ses_high', 'write']]\n",
    "print(mn_df.shape)\n",
    "mn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.899909\n",
      "         Iterations 6\n",
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                   prog   No. Observations:                  200\n",
      "Model:                        MNLogit   Df Residuals:                      192\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Fri, 26 Jun 2020   Pseudo R-squ.:                  0.1182\n",
      "Time:                        22:06:23   Log-Likelihood:                -179.98\n",
      "converged:                       True   LL-Null:                       -204.10\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.063e-08\n",
      "==============================================================================\n",
      "    prog=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.3660      1.174      2.015      0.044       0.065       4.667\n",
      "ses_middle     0.8247      0.490      1.683      0.092      -0.136       1.785\n",
      "ses_high       0.1802      0.648      0.278      0.781      -1.091       1.451\n",
      "write         -0.0557      0.023     -2.386      0.017      -0.101      -0.010\n",
      "------------------------------------------------------------------------------\n",
      "    prog=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.8522      1.166     -2.445      0.014      -5.138      -0.566\n",
      "ses_middle     0.5333      0.444      1.202      0.229      -0.336       1.403\n",
      "ses_high       1.1628      0.514      2.261      0.024       0.155       2.171\n",
      "write          0.0579      0.021      2.706      0.007       0.016       0.100\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exclude ses_low to make this column a baseline. \n",
    "# This prevents multicollinearity.\n",
    "X = mn_df[['ses_middle', 'ses_high', 'write']]\n",
    "\n",
    "y = mn_df['prog']\n",
    "\n",
    "mn_model = sm.MNLogit(y, sm.add_constant(X))\n",
    "mn_result = mn_model.fit()\n",
    "\n",
    "print(mn_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0         1\n",
      "const       10.654837  0.057718\n",
      "ses_middle   2.281160  1.704533\n",
      "ses_high     1.197411  3.198980\n",
      "write        0.945847  1.059639\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(mn_result.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "**Note: [This](https://stats.idre.ucla.edu/stata/output/multinomial-logistic-regression-2/) is a really good article for interpreting the coefficients!**\n",
    "\n",
    "You can see that there are two distinct rows, one signified by `prog=1` and another by `prog=2`. Multinomial logistic regression works by estimating $k - 1$ models, where $k$ is the number of levels in the dependent variable. \n",
    "\n",
    "Here, the base group is the general program type i.e., $y = 0$.\n",
    "\n",
    "Each row represents the two models that are estimated: vocational program type relative to general program type, and academic program type relative to general program type.\n",
    "\n",
    "In general, the coefficient estimates represent that for a unit change in the respective predictor, the logit of the respective outcome (vocational or academic program type) relative to the general program type is expected to change by however much the coefficient estimate is given the other variables are held constant. \n",
    "\n",
    "So, let's unpack some of these coefficients for `prog=1`. \n",
    "- If the student has a socioeconomic status of middle i.e., `ses_middle`, then the log-odds of that student being in the vocational program type relative to the general program type would increase by 0.8247. In other words, the odds for that student would **increase** by about 128%. \n",
    "- `write` has an estimate of -0.0557. The log-odds would decrease by 0.0557. When exponentiated, this becomes 0.945. So, for every unit increase in the student's writing score, the odds of that student being in the vocational program type relative to the general program type would **decrease** by about 0.95%. \n",
    "\n",
    "And again, it's the same idea with `prog=2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the results\n",
    "\n",
    "There are many different ways to evaluate the results of a logistic regression model. I'll go over two ways: a confusion matrix, and a ROC Curve.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is a matrix that summarizes the performance of a classification algorithm. It is a $N$x$N$ matrix, where $N$ is the number of classes.\n",
    "\n",
    "For simplicity, let's look at the confusion matrix for our binary logistic regression, and compare how my model from scratch compares with `statsmodels`. \n",
    "\n",
    "The confusion matrix would look something like this:\n",
    "\n",
    "|                 \t| Predicted Positive            \t| Predicted Negative             \t|\n",
    "|-----------------\t|-------------------------------\t|--------------------------------\t|\n",
    "| Actual Positive P\t| True Positive                 \t| False Negative (Type II Error) \t|\n",
    "| Actual Negative N\t| False Positive (Type I Error) \t| True Negative                  \t|\n",
    "\n",
    "\n",
    "This matrix allows us to compute more metrics:\n",
    "\n",
    "- **Accuracy**: $\\frac{TP + TN}{P + N}$\n",
    "    - Proportion of correct predictions in the entire data.\n",
    "- **Precision**: $\\frac{TP}{TP + FP}$\n",
    "    - Of all the positive predictions, how many were right?\n",
    "- **Specificity**: $\\frac{TN}{TN + FN}$\n",
    "    - Of all the negative predictions, how many were right?\n",
    "- **Recall/ Sensitivity**: $\\frac{TP}{TP + FN}$\n",
    "    - Of all the positive samples, how many were predicted correctly?\n",
    "- **F1 Score**: $2 \\cdot \\frac{prec. * recall}{prec. + recall}$\n",
    "    - Considering both precision and recall.\n",
    "    - Very useful for imbalanced data.\n",
    "    \n",
    "Let's calculate these metrics!\n",
    "\n",
    "### My Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decision_boundary(x, threshold = 0.5):\n",
    "    \"\"\"\n",
    "    Returns the classification of the sample for a given threshold\n",
    "    \"\"\"\n",
    "    return 1 if x >= threshold else 0\n",
    "\n",
    "# Deal with model from scratch\n",
    "X = bi_df[['ses_low', 'ses_middle', 'ses_high', 'write']]\n",
    "scratch_probs = bi_hypothesis(X, theta)\n",
    "\n",
    "scratch_preds = []\n",
    "for prob in scratch_probs:\n",
    "    scratch_preds.append(decision_boundary(prob))\n",
    "\n",
    "# Compare predictions and actuals\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for ind, y in enumerate(Y.flatten()):\n",
    "    if y == 1 and y == scratch_preds[ind]:\n",
    "        tp += 1\n",
    "    elif y == 0 and y == scratch_preds[ind]:\n",
    "        tn += 1\n",
    "    elif y == 1 and scratch_preds[ind] == 0:\n",
    "        fn += 1\n",
    "    elif y == 0 and scratch_preds[ind] == 1:\n",
    "        fp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 31\n",
      "True Negatives: 25\n",
      "False Positives: 20\n",
      "False Negatives: 19\n"
     ]
    }
   ],
   "source": [
    "print('True Positives: %d' % tp)\n",
    "print('True Negatives: %d' % tn)\n",
    "print('False Positives: %d' % fp)\n",
    "print('False Negatives: %d' % fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59\n",
      "Precision: 0.61\n",
      "Specificity: 0.57\n",
      "Recall/ Sensitivity: 0.62\n",
      "F1 Score: 0.61\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.2f' %  ((tp + tn) / len(X))  )\n",
    "print('Precision: %.2f' % (tp / (tp + fp)) )\n",
    "print('Specificity: %.2f' % (tn / (tn + fn)) )\n",
    "print('Recall/ Sensitivity: %.2f' % (tp / (tp + fn)) )\n",
    "print('F1 Score: %.2f' % ( 2 * (tp / ((tp + fp)) * (tp / (tp + fn))) / (tp / ((tp + fp)) + (tp / (tp + fn))) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `statsmodels` Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27. 18.]\n",
      " [15. 35.]]\n"
     ]
    }
   ],
   "source": [
    "bi_conf_mat = bi_result.pred_table()\n",
    "print(bi_conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, `pred_table[i, j]` refers to the number of times that class `i` was observed and the model predicted class `j`. Correct predictions are along the diagonal.\n",
    "\n",
    "Columns represent `j`, or the predicted values. The first column means the model predicted class 0, and the second column means that the model predicted class 1. \n",
    "\n",
    "Rows represent `i`, or the actual values. The first row means the sample is class 0, and the second row means that the sample is class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 35\n",
      "True Negatives: 27\n",
      "False Positives: 18\n",
      "False Negatives: 15\n"
     ]
    }
   ],
   "source": [
    "bi_tp = bi_conf_mat[1, 1]\n",
    "bi_tn = bi_conf_mat[0, 0]\n",
    "bi_fp = bi_conf_mat[0, 1]\n",
    "bi_fn = bi_conf_mat[1, 0]\n",
    "print('True Positives: %d' % bi_tp)\n",
    "print('True Negatives: %d' % bi_tn)\n",
    "print('False Positives: %d' % bi_fp)\n",
    "print('False Negatives: %d' % bi_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n",
      "Precision: 0.66\n",
      "Specificity: 0.64\n",
      "Recall/ Sensitivity: 0.70\n",
      "F1 Score: 0.68\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.2f' %  ((bi_tp + bi_tn) / len(X))  )\n",
    "print('Precision: %.2f' % (bi_tp / (bi_tp + bi_fp)) )\n",
    "print('Specificity: %.2f' % (bi_tn / (bi_tn + bi_fn)) )\n",
    "print('Recall/ Sensitivity: %.2f' % (bi_tp / (bi_tp + bi_fn)) )\n",
    "print('F1 Score: %.2f' % ( 2 * (bi_tp / ((bi_tp + bi_fp)) * (bi_tp / (bi_tp + bi_fn))) / (bi_tp / ((bi_tp + bi_fp)) + (bi_tp / (bi_tp + bi_fn))) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "What if we changed the threshold? If we were to alter our threshold, then our predictions would change. For example, increasing our threshold would increase specificity and decrease our recall. In other words, it would be harder for our model to make positive predictions.\n",
    "\n",
    "If we decreased our threshold, then the exact opposite would happen: our model would have an easier time making positive predictions.\n",
    "\n",
    "Why would you change the threshold? It depends on the problem. For instance, say we wanted a model that could predict whether or not a patient has cancer. Therefore, false negatives are really, really bad. We don't want to say a patient doesn't have cancer when in fact they do. So, we'd decrease the threshold. We'd be making more false positives, but our recall would increase and false positives aren't as terrible as false negatives. \n",
    "\n",
    "We can visually see the impact of changing the threshold using a ROC Curve. Let's do that for the `statsmodels` logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under Curve: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, plot_roc_curve\n",
    "\n",
    "X = bi_df[['ses_middle', 'ses_high', 'write']]\n",
    "probs = bi_result.predict(sm.add_constant(X))\n",
    "\n",
    "# Get fpr, tpr, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(Y, probs)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Area Under Curve: %.2f\" % roc_auc_score(Y, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC Curve is based on the True Positive Rate (TPR) on the Y-axis, and False Positive Rate (FPR) on the X-axis. \n",
    "\n",
    "- TPR/ Recall/ Sensitivity\n",
    "    - $\\frac{TP}{TP + FN}$\n",
    "    \n",
    "- FPR\n",
    "    - $1 -$ Specificity $= \\frac{FP}{TN + FP}$\n",
    "    - Of all the negative samples, how many did we predict wrong?\n",
    "\n",
    "This table shows what happens when we change threshold:\n",
    "\n",
    "| Threshold \t| Recall/ TPR \t| Specificity \t| FPR = 1 - Specificity \t|\n",
    "|-----------\t|-------------\t|-------------\t|-----------------------\t|\n",
    "| Increase  \t| Decrease    \t| Increase    \t| Decrease              \t|\n",
    "| Decrease  \t| Increase    \t| Decrease    \t| Increase              \t|\n",
    "\n",
    "\n",
    "The area under the curve tells us how much our model is capable of distinguishing between classes. In this case, it is 0.67. That means there is a 67% chance that the model will be able to distinguish between the positive and negative class. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
